import pandas as pd
import re
from config.settings import SPLIT_PATTERN
from src.utils import (clean_text, parse_refs, parse_subs,
                       normalize_pn_value, normalize_ref_designator,
                       check_spec_conflict)

def run_smt_comparison(df_bom, df_station, config, ignore_nc=False):
    results = []
    error_count = 0
    
    # 1. èšåˆç«™ä½è¡¨
    station_map = {}
    c_s_pn, c_s_ref, c_s_slot = config['st_pn'], config['st_ref'], config['st_slot']
    c_s_desc = config.get('st_desc')

    # ç«™ä½è¡¨å†…éƒ¨è¡¨å¤´/è¯´æ˜è¡Œå…³é”®å­—ï¼Œéœ€åœ¨èšåˆæ—¶å¿½ç•¥
    STATION_HEADER_TOKENS = {"å®‰è£…å·ç ", "å…ƒä»¶å", "å¤‡æ³¨", "å›¾æ ·å", "æ€»æ•°", "VERSION", "å®‰è£…å·", "ç«™ä½å·"}

    for idx, row in df_station.iterrows():
        excel_row = idx + 2
        raw_pn = row[c_s_pn]
        pn = normalize_pn_value(raw_pn)

        # å¿½ç•¥ç«™ä½è¡¨å†…éƒ¨çš„é‡å¤è¡¨å¤´è¡Œ / ç‰ˆæœ¬è¡Œï¼Œä¾‹å¦‚ "Version,1"ã€"å®‰è£…å·ç ,å…ƒä»¶å,å¤‡æ³¨..."
        row_str_vals = [str(v).strip() for v in row.values if pd.notna(v)]
        upper_vals = {v.upper() for v in row_str_vals}
        if (
            str(raw_pn).strip() in STATION_HEADER_TOKENS
            or pn in STATION_HEADER_TOKENS
            or "VERSION" in upper_vals
            or STATION_HEADER_TOKENS & set(row_str_vals)
        ):
            continue
        # åˆå¹¶ä½ç½®å·1ï¼ˆTé¢ï¼‰å’Œä½ç½®å·2ï¼ˆBé¢ï¼‰
        # ç«™ä½è¡¨å¯èƒ½æœ‰ä¸¤ç§æ ¼å¼ï¼š
        # 1. åˆ†ä¸¤è¡Œï¼šTé¢è¡Œï¼ˆä½ç½®å·1æœ‰å€¼ï¼Œä½ç½®å·2ä¸ºç©ºï¼‰å’ŒBé¢è¡Œï¼ˆä½ç½®å·1ä¸ºç©ºï¼Œä½ç½®å·2æœ‰å€¼ï¼‰
        # 2. åˆå¹¶ä¸€è¡Œï¼šä½ç½®å·1å’Œä½ç½®å·2åˆ†åˆ«å¡«å…¥å¯¹åº”é¢çš„ä½å·
        # 3. å¤šåˆ—ä½å·ï¼šå¯èƒ½æœ‰å¤šä¸ªä½å·åˆ—ï¼ˆå¦‚ Té¢ä½å·ã€Bé¢ä½å·ï¼‰ï¼Œéœ€è¦æ‹¼æ¥
        
        # å¤„ç†å¤šåˆ—ä½å·ï¼šéå† c_s_refï¼ˆç°åœ¨æ˜¯åˆ—è¡¨ï¼‰ï¼Œæ‹¼æ¥å„åˆ—çš„å€¼
        ref_parts = []
        if isinstance(c_s_ref, list):
            for ref_col in c_s_ref:
                val = row[ref_col]
                if pd.notna(val):
                    val_str = str(val).strip()
                    if val_str and val_str.upper() != 'NAN':  # è¿‡æ»¤ç©ºå€¼å’Œ NaN
                        ref_parts.append(val_str)
        else:
            # å…¼å®¹å•åˆ—çš„æƒ…å†µï¼ˆå­—ç¬¦ä¸²ï¼‰
            val = row[c_s_ref]
            if pd.notna(val):
                val_str = str(val).strip()
                if val_str and val_str.upper() != 'NAN':
                    ref_parts.append(val_str)
        
        # å°†å¤šåˆ—ä½å·æ‹¼æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œç”¨ç©ºæ ¼åˆ†éš”
        combined_ref = " ".join(ref_parts)
        refs = parse_refs(combined_ref, SPLIT_PATTERN)
        st_desc_val = str(row[c_s_desc]).strip() if c_s_desc and pd.notna(row[c_s_desc]) else ""

        if not pn and refs:
            error_count += 1
            results.append({
                "çº§åˆ«": "ğŸ”´ ä¸¥é‡", "æ ¸å¯¹ç»“æœ": "æ•°æ®é”™è¯¯", "åŸå§‹è¡Œå·": f"Station: {excel_row}",
                "BOMæ–™å·": "UNKNOWN", "å·®å¼‚è¯´æ˜": "âŒ ç«™ä½è¡¨æœ‰ä½å·æ— æ–™å·", "ç«™ä½å·": "", "BOMæ•°é‡": 0, "å®é™…æ•°é‡": len(refs),
                "BOMæè¿°": "", "ç«™ä½å¤‡æ³¨": st_desc_val
            })
            continue
        if not pn: continue

        slot = str(row[c_s_slot]).strip() if c_s_slot else ""
        if pn not in station_map: station_map[pn] = {'refs': set(), 'slots': [], 'desc': st_desc_val, 'rows': []}
        station_map[pn]['refs'].update(refs)
        station_map[pn]['rows'].append(str(excel_row))
        if slot and slot not in station_map[pn]['slots']: station_map[pn]['slots'].append(slot)
        if st_desc_val and not station_map[pn]['desc']: station_map[pn]['desc'] = st_desc_val

    # 2. èšåˆ BOM
    bom_aggregated = {}
    c_b_pn, c_b_ref = config['bom_pn'], config['bom_ref']
    c_b_sub, c_b_desc = config['bom_sub'], config['bom_desc']

    for idx, row in df_bom.iterrows():
        excel_row = idx + 2
        bom_pn = normalize_pn_value(row[c_b_pn])
        # åˆå¹¶ä½ç½®å·1ï¼ˆTé¢ï¼‰å’Œä½ç½®å·2ï¼ˆBé¢ï¼‰
        # åŒä¸€ç‰©æ–™å¯èƒ½åˆ†ä¸¤è¡Œï¼šTé¢ï¼ˆä½ç½®å·1æœ‰å€¼ï¼Œä½ç½®å·2ä¸ºç©ºï¼‰å’ŒBé¢ï¼ˆä½ç½®å·1ä¸ºç©ºï¼Œä½ç½®å·2æœ‰å€¼ï¼‰
        # æˆ–è€…æœ‰å¤šä¸ªä½å·åˆ—ï¼ˆå¦‚ Té¢ä½å·ã€Bé¢ä½å·ï¼‰ï¼Œéœ€è¦æ‹¼æ¥
        
        # å¤„ç†å¤šåˆ—ä½å·ï¼šéå† c_b_refï¼ˆç°åœ¨æ˜¯åˆ—è¡¨ï¼‰ï¼Œæ‹¼æ¥å„åˆ—çš„å€¼
        ref_parts = []
        if isinstance(c_b_ref, list):
            for ref_col in c_b_ref:
                val = row[ref_col]
                if pd.notna(val):
                    val_str = str(val).strip()
                    if val_str and val_str.upper() != 'NAN':  # è¿‡æ»¤ç©ºå€¼å’Œ NaN
                        ref_parts.append(val_str)
        else:
            # å…¼å®¹å•åˆ—çš„æƒ…å†µï¼ˆå­—ç¬¦ä¸²ï¼‰
            val = row[c_b_ref]
            if pd.notna(val):
                val_str = str(val).strip()
                if val_str and val_str.upper() != 'NAN':
                    ref_parts.append(val_str)
        
        # å°†å¤šåˆ—ä½å·æ‹¼æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œç”¨ç©ºæ ¼åˆ†éš”
        combined_ref = " ".join(ref_parts)
        bom_refs = parse_refs(combined_ref, SPLIT_PATTERN)
        
        if ignore_nc and not bom_refs: pass
        
        if not bom_pn and bom_refs:
            error_count += 1
            results.append({
                "çº§åˆ«": "ğŸ”´ ä¸¥é‡", "æ ¸å¯¹ç»“æœ": "æ•°æ®é”™è¯¯", "åŸå§‹è¡Œå·": f"BOM: {excel_row}",
                "BOMæ–™å·": "MISSING", "å·®å¼‚è¯´æ˜": "âŒ BOMè¡Œç¼ºå¤±æ–™å·", "ç«™ä½å·": "", "BOMæ•°é‡": len(bom_refs), "å®é™…æ•°é‡": 0,
                "BOMæè¿°": "", "ç«™ä½å¤‡æ³¨": ""
            })
            continue
        if not bom_pn: continue

        if bom_pn not in bom_aggregated:
            subs = parse_subs(row[c_b_sub], SPLIT_PATTERN) if c_b_sub else []
            desc = str(row[c_b_desc]).strip() if c_b_desc else ""
            bom_aggregated[bom_pn] = {'refs': set(), 'subs': set(subs), 'desc': desc, 'rows': []}
        
        if bom_refs: bom_aggregated[bom_pn]['refs'].update(bom_refs)
        bom_aggregated[bom_pn]['rows'].append(str(excel_row))
        if c_b_sub: bom_aggregated[bom_pn]['subs'].update(parse_subs(row[c_b_sub], SPLIT_PATTERN))

    # 3. æ­£å‘æ¯”å¯¹
    claimed_st_pns = set()
    for bom_pn, bom_data in bom_aggregated.items():
        bom_refs = bom_data['refs']
        bom_desc = bom_data['desc']
        bom_subs = list(bom_data['subs'])
        row_str = ",".join(bom_data['rows'][:3]) + ("..." if len(bom_data['rows'])>3 else "")
        
        if not bom_refs:
            if ignore_nc:
                results.append({
                    "çº§åˆ«": "âšª å¿½ç•¥", "æ ¸å¯¹ç»“æœ": "NC/è·³è¿‡", "åŸå§‹è¡Œå·": f"BOM: {row_str}",
                    "BOMæ–™å·": bom_pn, "å·®å¼‚è¯´æ˜": "â„¹ï¸ NC", "ç«™ä½å·": "", "BOMæ•°é‡": 0, "å®é™…æ•°é‡": 0,
                    "BOMæè¿°": bom_desc, "ç«™ä½å¤‡æ³¨": ""
                })
            else:
                error_count += 1
                results.append({
                    "çº§åˆ«": "ğŸŸ  è­¦å‘Š", "æ ¸å¯¹ç»“æœ": "ä½å·ä¸ºç©º", "åŸå§‹è¡Œå·": f"BOM: {row_str}",
                    "BOMæ–™å·": bom_pn, "å·®å¼‚è¯´æ˜": "âš ï¸ ä½å·ä¸ºç©º", "ç«™ä½å·": "", "BOMæ•°é‡": 0, "å®é™…æ•°é‡": 0,
                    "BOMæè¿°": bom_desc, "ç«™ä½å¤‡æ³¨": ""
                })
            continue 
        
        targets = [bom_pn] + bom_subs
        found_refs = set()
        found_slots = []
        matched_pns = []
        found_st_descs = []

        for target in targets:
            if target in station_map:
                matched_pns.append(target)
                found_refs.update(station_map[target]['refs'])
                found_slots.extend(station_map[target]['slots'])
                found_st_descs.append(station_map[target]['desc'])
                claimed_st_pns.add(target)

        slots_str = ",".join(sorted(list(set(found_slots))))
        st_desc_str = " | ".join([d for d in set(found_st_descs) if d])

        norm_bom = {normalize_ref_designator(r): r for r in bom_refs}
        norm_found = {normalize_ref_designator(r): r for r in found_refs}
        set_bom = set(norm_bom.keys())
        set_found = set(norm_found.keys())

        # å‡†å¤‡ç›´è§‚å±•ç¤ºçš„ä½å·æ˜ç»†ï¼ˆæœªç»å½’ä¸€åŒ–ï¼Œç”¨äº UI é¢„è§ˆï¼‰
        bom_refs_display = ",".join(sorted(list(bom_refs))) if bom_refs else ""
        found_refs_display = ",".join(sorted(list(found_refs))) if found_refs else ""

        if not matched_pns:
            level = "ğŸ”´ ä¸¥é‡"
            status = "ç¼ºæ–™"
            detail = "âŒ ç«™ä½è¡¨ä¸­æœªæ‰¾åˆ°ä¸»æ–™æˆ–æ›¿ä»£æ–™"
            error_count += 1
        else:
            missing = set_bom - set_found
            extra = set_found - set_bom
            if not missing and not extra:
                is_conf, conf_msg = check_spec_conflict(bom_desc, st_desc_str)
                if is_conf:
                    level, status, detail = "ğŸŸ  è­¦å‘Š", "è§„æ ¼é¢„è­¦", f"âš ï¸ {conf_msg}"
                    error_count += 1
                else:
                    level, status, detail = "ğŸŸ¢ æ­£å¸¸", "é€šè¿‡", "åŒ¹é…æˆåŠŸ"
                if bom_pn not in matched_pns: detail += " (ä½¿ç”¨æ›¿ä»£æ–™)"
            else:
                level, status = "ğŸŸ  è­¦å‘Š", "ä½å·ä¸ç¬¦"
                msgs = []
                if missing: msgs.append(f"æ¼è´´({len(missing)}): {','.join([norm_bom[k] for k in missing])}")
                if extra: msgs.append(f"å¤šè´´({len(extra)}): {','.join([norm_found[k] for k in extra])}")
                detail = " | ".join(msgs)
                error_count += 1

        results.append({
            "çº§åˆ«": level,
            "æ ¸å¯¹ç»“æœ": status,
            "åŸå§‹è¡Œå·": f"BOM: {row_str}",
            "BOMæ–™å·": bom_pn,
            "BOMæè¿°": bom_desc,
            "ç«™ä½å¤‡æ³¨": st_desc_str,
            "å·®å¼‚è¯´æ˜": detail,
            "ç«™ä½å·": slots_str,
            "BOMæ•°é‡": len(bom_refs),
            "å®é™…æ•°é‡": len(found_refs),
            # æ–°å¢ä¸¤åˆ—ï¼šç”¨äºåœ¨ç»“æœé¢„è§ˆä¸­ç›´è§‚å¯¹æ¯” BOM vs Station ä½å·
            "BOMä½å·æ˜ç»†": bom_refs_display,
            "å®è£…ä½å·æ˜ç»†": found_refs_display,
        })

    # 4. åå‘æ£€æµ‹
    for extra_pn in (set(station_map.keys()) - claimed_st_pns):
        st_data = station_map[extra_pn]
        row_str = ",".join(st_data['rows'][:3])
        error_count += 1
        results.append({
            "çº§åˆ«": "ğŸ”´ ä¸¥é‡",
            "æ ¸å¯¹ç»“æœ": "é”™æ–™/å¤šä½™",
            "åŸå§‹è¡Œå·": f"Station: {row_str}...",
            "BOMæ–™å·": "N/A",
            "BOMæè¿°": "",
            "ç«™ä½å¤‡æ³¨": st_data['desc'],
            "å·®å¼‚è¯´æ˜": f"âŒ éæ³•ç‰©æ–™: {extra_pn}",
            "ç«™ä½å·": ",".join(set(st_data['slots'])),
            "BOMæ•°é‡": 0,
            "å®é™…æ•°é‡": len(st_data['refs']),
            "BOMä½å·æ˜ç»†": "",
            "å®è£…ä½å·æ˜ç»†": ",".join(sorted(list(st_data['refs']))),
        })

    return results, error_count, len(bom_aggregated)


# --- é€šç”¨åˆ—è¡¨ç»“æ„æ¯”å¯¹ç±»ï¼ˆBOM_Data / Station_Dataï¼‰ ---

class SMTComparator:
    """
    é€šç”¨ SMT é˜²é”™æ¯”å¯¹ç±»ã€‚

    è¾“å…¥:
        BOM_Data: List[dict]ï¼Œå­—æ®µ:
            - main_part: ä¸»æ–™å· (str)
            - alt_part:  æ›¿ä»£æ–™å· (str, å¯ä¸ºç©º)
            - description: è§„æ ¼æè¿° (str)
            - refs: åŸå§‹ä½å·å­—ç¬¦ä¸² (str, é€—å·åˆ†éš”ï¼Œå¦‚ "C1,C2\\n")

        Station_Data: List[dict]ï¼Œå­—æ®µ:
            - part_no: æ–™å· (str)
            - slot: ç«™ä½ä½ç½® (str)
            - comment: æœºå™¨å¤‡æ³¨ (str)
            - refs: åŸå§‹ä½å·å­—ç¬¦ä¸² (str, æ–œæ åˆ†éš”ï¼Œå¦‚ "C1/C2")

    compare() è¿”å›:
        List[dict]ï¼Œæ¯æ¡è®°å½•åŒ…å«:
            - level:   'FAIL' / 'WARN' / 'PASS'
            - code:    'MISSING_FEEDER' / 'MISSING_REFS' /
                       'WARN_SPEC' / 'UNKNOWN_PART' / 'OK'
            - message: æ–‡æœ¬è¯´æ˜
            - context: é™„åŠ ä¿¡æ¯ (å­—å…¸)
    """

    SPEC_PATTERN = re.compile(r"\d+\s*[KkMmVv]")

    def _clean_refs(self, raw: str, sep: str) -> set:
        """æŒ‰ç»™å®šåˆ†éš”ç¬¦æ¸…æ´—ä½å· -> Setï¼Œä½¿ç”¨é¡¹ç›®ç»Ÿä¸€çš„ clean_text ä¸ normalize_ref_designatorã€‚"""
        if raw is None:
            return set()
        # åŸºç¡€æ¸…æ´—ï¼šå»ç©ºã€è½¬å¤§å†™ã€å»ä¸å¯è§å­—ç¬¦
        text = clean_text(raw)
        if not text:
            return set()
        # å»å¼•å·ä¸æ¢è¡Œ
        text = text.replace('"', '').replace("\n", "").replace("\r", "")
        parts = [p.strip() for p in text.split(sep) if p.strip()]
        # ä½¿ç”¨ normalize_ref_designator åšä½å·å½’ä¸€åŒ–ï¼Œé˜²æ­¢ C1 / C-1 ä¸ä¸€è‡´
        return set(normalize_ref_designator(p) for p in parts)

    def _standardize_bom(self, bom_list):
        """ç”Ÿæˆ BOM æ ‡å‡†åŒ–ç»“æ„: [{'main', 'alt', 'desc', 'ref_set', 'raw_refs_map'}]"""
        std = []
        for item in bom_list or []:
            main = normalize_pn_value(item.get("main_part", ""))
            alt = normalize_pn_value(item.get("alt_part", ""))
            desc = str(item.get("description", "") or "").strip()
            raw_refs = str(item.get("refs", "") or "")

            ref_norm_set = self._clean_refs(raw_refs, sep=",")
            # ä¿å­˜è§„èŒƒä½å· -> åŸå§‹ä½å·æ˜ å°„ï¼Œç”¨äºåç»­æŠ¥è¡¨å±•ç¤º
            raw_map = {}
            if raw_refs:
                text = clean_text(raw_refs).replace('"', '').replace("\n", "").replace("\r", "")
                for p in [p.strip() for p in text.split(",") if p.strip()]:
                    key = normalize_ref_designator(p)
                    raw_map[key] = p

            std.append(
                {
                    "main": main,
                    "alt": alt,
                    "desc": desc,
                    "ref_set": ref_norm_set,
                    "raw_refs_map": raw_map,
                }
            )
        return std

    def _aggregate_station(self, station_list):
        """
        ç”Ÿæˆç«™ä½èšåˆç»“æ„:
            {
              'PN1': {
                  'ref_set': {...},        # è§„èŒƒåŒ–ä½å·é›†åˆ
                  'raw_refs_map': {...},   # è§„èŒƒä½å· -> åŸå§‹ä½å·
                  'slots': [..],           # æ‰€æœ‰åˆ†ç›˜ç«™ä½
                  'comments': set([...])   # å¤‡æ³¨é›†åˆ
              },
              ...
            }
        """
        agg = {}
        for item in station_list or []:
            pn = normalize_pn_value(item.get("part_no", ""))
            if not pn:
                continue

            slot = str(item.get("slot", "") or "").strip()
            comment = str(item.get("comment", "") or "").strip()
            raw_refs = str(item.get("refs", "") or "")

            ref_norm_set = self._clean_refs(raw_refs, sep="/")

            # æ„é€ åŸå§‹ä½å·æ˜ å°„
            raw_map = {}
            if raw_refs:
                text = clean_text(raw_refs).replace('"', '').replace("\n", "").replace("\r", "")
                for p in [p.strip() for p in text.split("/") if p.strip()]:
                    key = normalize_ref_designator(p)
                    raw_map[key] = p

            if pn not in agg:
                agg[pn] = {
                    "ref_set": set(),
                    "raw_refs_map": {},
                    "slots": [],
                    "comments": set(),
                }

            agg[pn]["ref_set"].update(ref_norm_set)
            agg[pn]["raw_refs_map"].update(raw_map)
            if slot:
                agg[pn]["slots"].append(slot)
            if comment:
                agg[pn]["comments"].add(comment)

        return agg

    def _extract_spec_tokens(self, text: str):
        """ä»æè¿°/å¤‡æ³¨é‡Œæå–å…³é”®è§„æ ¼ tokenï¼ˆå¦‚ 10K / 4.7M / 25Vï¼‰ã€‚"""
        if not text:
            return set()
        return set(self.SPEC_PATTERN.findall(text))

    def compare(self, bom_list, station_list):
        """
        ä¸»å…¥å£ï¼šæ‰§è¡Œæ­£å‘+åå‘æ¯”å¯¹ã€‚
        è¿”å› List[dict]ï¼Œæ¯æ¡åŒ…å« level / code / message / contextã€‚
        """
        results = []

        bom_std = self._standardize_bom(bom_list)
        st_agg = self._aggregate_station(station_list)

        # --- æ­£å‘æ¯”å¯¹ï¼šéå† BOMï¼ŒæŸ¥æ¼ã€è§„æ ¼é¢„è­¦ ---
        for item in bom_std:
            main = item["main"]
            alt = item["alt"]
            desc = item["desc"]
            bom_refs = item["ref_set"]

            # 1) æ–™å·æ˜¯å¦ä¸Šæ–™
            candidates = []
            if main:
                candidates.append(main)
            if alt:
                candidates.append(alt)

            installed_refs = set()
            comments_joined = ""
            used_parts = []

            for p in candidates:
                if p in st_agg:
                    used_parts.append(p)
                    installed_refs.update(st_agg[p]["ref_set"])

            if not used_parts:
                results.append(
                    {
                        "level": "FAIL",
                        "code": "MISSING_FEEDER",
                        "message": "ç«™ä½è¡¨ä¸­æœªæ‰¾åˆ°ä¸»æ–™æˆ–æ›¿ä»£æ–™",
                        "context": {
                            "main_part": main,
                            "alt_part": alt,
                            "bom_desc": desc,
                        },
                    }
                )
                # ç¼ºæ–™å·²æ˜¯ä¸¥é‡é”™è¯¯ï¼Œæœ¬æ¡æ— éœ€ç»§ç»­ä½å·æ¯”å¯¹ä¸è§„æ ¼é¢„è­¦
                continue

            # æ±‡æ€» remarkï¼Œç”¨äºè§„æ ¼é˜²å‘†
            all_comments = set()
            for p in used_parts:
                all_comments.update(st_agg[p]["comments"])
            comments_joined = " | ".join(all_comments)

            # 2) ä½å·é›†åˆè¿ç®—ï¼šBOM - å®è£… = æ¼è´´
            missing_refs = bom_refs - installed_refs
            if missing_refs:
                # è¿˜åŸæˆåŸå§‹ä½å·ç”¨äºå±•ç¤º
                raw_missing = [
                    item["raw_refs_map"].get(r, r) for r in sorted(missing_refs)
                ]
                results.append(
                    {
                        "level": "FAIL",
                        "code": "MISSING_REFS",
                        "message": f"å­˜åœ¨æ¼è´´ä½å·: {','.join(raw_missing)}",
                        "context": {
                            "main_part": main,
                            "alt_part": alt,
                            "missing_refs": raw_missing,
                        },
                    }
                )

            # 3) è§„æ ¼é˜²å‘†ï¼šBOM è§„æ ¼ token éœ€åœ¨ç«™ä½å¤‡æ³¨ä¸­èƒ½æ‰¾åˆ°è‡³å°‘ä¸€ä¸ª
            bom_tokens = self._extract_spec_tokens(desc)
            if bom_tokens:
                st_tokens = self._extract_spec_tokens(comments_joined)
                if not (bom_tokens & st_tokens):
                    results.append(
                        {
                            "level": "WARN",
                            "code": "WARN_SPEC",
                            "message": "BOM è§„æ ¼åœ¨ç«™ä½å¤‡æ³¨ä¸­æœªåŒ¹é…åˆ°å…³é”®å‚æ•°",
                            "context": {
                                "main_part": main,
                                "alt_part": alt,
                                "bom_desc": desc,
                                "station_comment": comments_joined,
                                "bom_tokens": sorted(bom_tokens),
                                "station_tokens": sorted(st_tokens),
                            },
                        }
                    )

            # è‹¥æ²¡æœ‰æ¼è´´ä¸”æ²¡æœ‰è§„æ ¼å‘Šè­¦ï¼Œåˆ™ç»™ä¸€æ¡ PASS è®°å½•ï¼ˆå¯é€‰ï¼‰
            if not missing_refs:
                results.append(
                    {
                        "level": "PASS",
                        "code": "OK",
                        "message": "BOM ä¸ç«™ä½è¡¨æ¯”å¯¹é€šè¿‡",
                        "context": {
                            "main_part": main,
                            "alt_part": alt,
                            "bom_desc": desc,
                        },
                    }
                )

        # --- åå‘æ¯”å¯¹ï¼šéå†ç«™ä½è¡¨ï¼ŒæŸ¥æœªçŸ¥ç‰©æ–™ ---
        valid_bom_parts = set()
        for item in bom_std:
            if item["main"]:
                valid_bom_parts.add(item["main"])
            if item["alt"]:
                valid_bom_parts.add(item["alt"])

        for part_no, data in st_agg.items():
            if part_no not in valid_bom_parts:
                results.append(
                    {
                        "level": "FAIL",
                        "code": "UNKNOWN_PART",
                        "message": "ç«™ä½è¡¨å‘ç° BOM æœªå£°æ˜çš„ç‰©æ–™",
                        "context": {
                            "part_no": part_no,
                            "slots": sorted(set(data["slots"])),
                            "refs": [
                                data["raw_refs_map"].get(r, r)
                                for r in sorted(data["ref_set"])
                            ],
                        },
                    }
                )

        return results
